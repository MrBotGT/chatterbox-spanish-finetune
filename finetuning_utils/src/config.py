from dataclasses import dataclass

@dataclass
class TrainConfig:
    # --- Paths ---
    # Directory where setup.py downloaded the files
    model_dir: str = "./pretrained_models"
    
    # Path to your metadata CSV (Format: ID|RawText|NormText)
    csv_path: str = "../chatterbox_dataset_2/metadata.csv"

    # Directory containing WAV files
    wav_dir: str = "../chatterbox_dataset_2/wavs"

    preprocessed_dir = "../chatterbox_dataset_2/preprocess"
    
    # Output directory for the finetuned model
    output_dir: str = "../models/turbo"

    ljspeech = True # Set True if the dataset format is ljspeech, and False if it's file-based.
    preprocess = False # Ya preprocesado

    is_turbo: bool = True # Modo Turbo para espa√±ol con vocab extendido

    # --- Vocabulary ---
    # The size of the NEW vocabulary (from tokenizer.json)
    # Ensure this matches the JSON file generated by your tokenizer script.
    # For Turbo mode: Use the exact number provided by setup.py (e.g., 52260)
    new_vocab_size: int = 52260 if is_turbo else 2454 

    # --- Hyperparameters ---
    batch_size: int = 4         # 4 for 24GB VRAM (RunPod), 1 for Mac
    grad_accum: int = 2        # Effective Batch Size = 4 * 2 = 8
    learning_rate: float = 5e-5 # T3 is sensitive, keep low
    num_epochs: int = 100       # Sufficient for Spanish fine-tuning

    # --- Constraints ---
    start_text_token = 255
    stop_text_token = 0
    max_text_len: int = 256
    max_speech_len: int = 850   # Full length for CUDA
    prompt_duration: float = 3.0 # Duration for the reference prompt (seconds)
